{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5145ab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run in Colab or local env\n",
    "%pip install -q tensorflow matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309c2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d52023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14\n",
      "Word index sample: [('<OOV>', 1), ('i', 2), ('love', 3), ('deep', 4), ('learning', 5), ('artificial', 6), ('intelligence', 7), ('is', 8), ('models', 9), ('fun', 10)]\n",
      "Sequences: [[2, 3, 4, 5], [2, 3, 6, 7], [4, 5, 8, 10], [6, 7, 8, 11], [2, 3, 9], [9, 12, 13]]\n",
      "Example input (padded): [0 0 0 2] target: 3\n"
     ]
    }
   ],
   "source": [
    "# Simple toy corpus (small sentences)\n",
    "corpus = [\n",
    "    \"i love deep learning\",\n",
    "    \"i love artificial intelligence\",\n",
    "    \"deep learning is fun\",\n",
    "    \"artificial intelligence is cool\",\n",
    "    \"i love models\",\n",
    "    \"models learn patterns\"\n",
    "]\n",
    "\n",
    "# Build tokenizer (word-level for simplicity)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=True, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {i:w for w,i in word_index.items()}\n",
    "\n",
    "vocab_size = len(word_index) + 1  # +1 for padding 0\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Word index sample:\", list(word_index.items())[:10])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "print(\"Sequences:\", sequences)\n",
    "\n",
    "# For next-token prediction prepare input->target pairs (shifted sequences)\n",
    "inputs = []\n",
    "targets = []\n",
    "max_len = max(len(s) for s in sequences)\n",
    "\n",
    "for s in sequences:\n",
    "    for i in range(1, len(s)):\n",
    "        inputs.append(s[:i])       # tokens [0..i-1]\n",
    "        targets.append(s[i])       # next token\n",
    "\n",
    "# Pad inputs\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_len, padding='pre')\n",
    "targets = np.array(targets)\n",
    "\n",
    "print(\"Example input (padded):\", inputs[0], \"target:\", targets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d7ef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (2, 4, 16)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 16  # small, for demonstration\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "# Test embedding\n",
    "sample_ids = tf.constant(inputs[:2])\n",
    "emb = embedding_layer(sample_ids)\n",
    "print(\"Embedding shape:\", emb.shape)  # (batch, seq_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b7864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos encoding shape: (1, 4, 16)\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]  # (seq_len, 1)\n",
    "    i = np.arange(d_model)[np.newaxis, :]    # (1, d_model)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates  # (seq_len, d_model)\n",
    "\n",
    "    # apply sin to even indices and cos to odd indices\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    coses = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.zeros(angle_rads.shape)\n",
    "    pos_encoding[:, 0::2] = sines\n",
    "    pos_encoding[:, 1::2] = coses\n",
    "    return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)  # shape (1, seq_len, d_model)\n",
    "\n",
    "# Example:\n",
    "pos_enc = positional_encoding(max_len, embed_dim)\n",
    "print(\"Pos encoding shape:\", pos_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb16a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    # Q, K, V shapes: (batch, heads, seq_len, depth)\n",
    "    matmul_qk = tf.matmul(Q, K, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "    scaled_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, V)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c74d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.Wq = tf.keras.layers.Dense(d_model)\n",
    "        self.Wk = tf.keras.layers.Dense(d_model)\n",
    "        self.Wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x shape: (batch, seq_len, d_model) -> (batch, num_heads, seq_len, depth)\n",
    "        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        Q = self.Wq(x)  # (batch, seq, d_model)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        # attn_output: (batch, num_heads, seq_len, depth)\n",
    "        attn_output = tf.transpose(attn_output, perm=[0, 2, 1, 3])  # (batch, seq_len, num_heads, depth)\n",
    "        concat = tf.reshape(attn_output, (tf.shape(attn_output)[0], tf.shape(attn_output)[1], self.d_model))  # (batch, seq_len, d_model)\n",
    "        output = self.dense(concat)\n",
    "        return output, attn_weights  # attn_weights: (batch, num_heads, seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4183144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        attn_output, attn_weights = self.mha(x, mask=mask)  # (batch, seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # residual\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # residual\n",
    "\n",
    "        return out2, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab8ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_len, d_model=16, num_heads=2, num_layers=2, dff=64, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_len, d_model)\n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.final_dense = tf.keras.layers.Dense(vocab_size)  # predict next token id logits\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        # x: (batch, seq_len)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)  # (batch, seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # scale embeddings\n",
    "        x += self.pos_encoding[:, -seq_len:, :]  # broadcast, add positional encoding\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        all_attn_weights = []\n",
    "        for enc_layer in self.enc_layers:\n",
    "            x, attn_weights = enc_layer(x, training=training, mask=mask)\n",
    "            all_attn_weights.append(attn_weights)\n",
    "\n",
    "        logits = self.final_dense(x)  # (batch, seq_len, vocab_size)\n",
    "        return logits, all_attn_weights  # return attention weights from each layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65ff2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (16, 4) y_labels shape: (16, 4)\n"
     ]
    }
   ],
   "source": [
    "# inputs: list of padded sequences; targets: next-token ids\n",
    "inputs = np.array(inputs)  # (N_examples, seq_len)\n",
    "targets = targets  # (N_examples,)\n",
    "\n",
    "# We'll train with batches; our model outputs logits for each position,\n",
    "# but we will only train on the last (rightmost) non-pad position.\n",
    "# Find index of first non-zero token in each padded input to locate last real token.\n",
    "seq_len = inputs.shape[1]\n",
    "\n",
    "# Build sample weights and labels that only consider last real token (mask)\n",
    "y = []  # expected labels will be shaped (N, seq_len) but we zero-out except last token\n",
    "y_masked = np.zeros((inputs.shape[0], seq_len), dtype=np.int32)  # we will encode -1 where unused\n",
    "y_labels = np.full((inputs.shape[0], seq_len), -1)  # -1 means ignore in loss\n",
    "\n",
    "for i, inp in enumerate(inputs):\n",
    "    # find index of last non-zero\n",
    "    nonzeros = np.where(inp != 0)[0]\n",
    "    if len(nonzeros) == 0:\n",
    "        last_idx = seq_len - 1\n",
    "    else:\n",
    "        last_idx = nonzeros[-1]  # last real token index\n",
    "    # We want to predict the token that follows this input. Our 'target' variable already gives next token id.\n",
    "    # We'll place it at position last_idx (so model predicts target at that position)\n",
    "    y_labels[i, last_idx] = targets[i]\n",
    "\n",
    "# ✅ Fix dtype mismatch\n",
    "y_labels = y_labels.astype('int32')\n",
    "\n",
    "# We'll use a custom loss that only counts positions where label != -1\n",
    "print(\"inputs shape:\", inputs.shape, \"y_labels shape:\", y_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b8f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    # y_true: (batch, seq_len) with -1 where ignore; y_pred: (batch, seq_len, vocab)\n",
    "    mask = tf.not_equal(y_true, -1)\n",
    "    y_true_filtered = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_filtered = tf.boolean_mask(y_pred, mask)\n",
    "    loss = loss_object(y_true_filtered, y_pred_filtered)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_accuracy(y_true, y_pred):\n",
    "    mask = tf.not_equal(y_true, -1)\n",
    "    y_true_filtered = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_filtered = tf.boolean_mask(y_pred, mask)\n",
    "    pred_ids = tf.argmax(y_pred_filtered, axis=-1, output_type=tf.int32)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(pred_ids, y_true_filtered), tf.float32))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9064c538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 — loss: 3.0976 — acc: 0.0625\n",
      "Epoch 2/1000 — loss: 2.8945 — acc: 0.1250\n",
      "Epoch 3/1000 — loss: 2.7267 — acc: 0.1875\n",
      "Epoch 4/1000 — loss: 2.7012 — acc: 0.1250\n",
      "Epoch 5/1000 — loss: 2.8589 — acc: 0.0000\n",
      "Epoch 6/1000 — loss: 2.5995 — acc: 0.1250\n",
      "Epoch 7/1000 — loss: 2.5731 — acc: 0.1875\n",
      "Epoch 8/1000 — loss: 2.7244 — acc: 0.1875\n",
      "Epoch 9/1000 — loss: 2.3657 — acc: 0.2500\n",
      "Epoch 10/1000 — loss: 2.5428 — acc: 0.1875\n",
      "Epoch 11/1000 — loss: 2.3958 — acc: 0.3125\n",
      "Epoch 12/1000 — loss: 2.2990 — acc: 0.2500\n",
      "Epoch 13/1000 — loss: 2.3705 — acc: 0.1250\n",
      "Epoch 14/1000 — loss: 2.4779 — acc: 0.1875\n",
      "Epoch 15/1000 — loss: 2.6996 — acc: 0.1250\n",
      "Epoch 16/1000 — loss: 2.5813 — acc: 0.2500\n",
      "Epoch 17/1000 — loss: 2.3731 — acc: 0.1875\n",
      "Epoch 18/1000 — loss: 2.3593 — acc: 0.1875\n",
      "Epoch 19/1000 — loss: 2.1778 — acc: 0.2500\n",
      "Epoch 20/1000 — loss: 2.4294 — acc: 0.1875\n",
      "Epoch 21/1000 — loss: 2.1188 — acc: 0.5000\n",
      "Epoch 22/1000 — loss: 2.1915 — acc: 0.2500\n",
      "Epoch 23/1000 — loss: 2.2268 — acc: 0.1875\n",
      "Epoch 24/1000 — loss: 2.1242 — acc: 0.3125\n",
      "Epoch 25/1000 — loss: 2.0513 — acc: 0.3125\n",
      "Epoch 26/1000 — loss: 2.2704 — acc: 0.1250\n",
      "Epoch 27/1000 — loss: 2.1969 — acc: 0.2500\n",
      "Epoch 28/1000 — loss: 2.0975 — acc: 0.4375\n",
      "Epoch 29/1000 — loss: 2.0249 — acc: 0.4375\n",
      "Epoch 30/1000 — loss: 1.8375 — acc: 0.4375\n",
      "Epoch 31/1000 — loss: 1.9104 — acc: 0.5000\n",
      "Epoch 32/1000 — loss: 2.0412 — acc: 0.3750\n",
      "Epoch 33/1000 — loss: 1.9833 — acc: 0.5000\n",
      "Epoch 34/1000 — loss: 1.8304 — acc: 0.3750\n",
      "Epoch 35/1000 — loss: 1.6617 — acc: 0.6250\n",
      "Epoch 36/1000 — loss: 1.8129 — acc: 0.4375\n",
      "Epoch 37/1000 — loss: 1.4585 — acc: 0.6250\n",
      "Epoch 38/1000 — loss: 1.5587 — acc: 0.6250\n",
      "Epoch 39/1000 — loss: 1.3812 — acc: 0.7500\n",
      "Epoch 40/1000 — loss: 1.5439 — acc: 0.5000\n",
      "Epoch 41/1000 — loss: 1.3791 — acc: 0.6250\n",
      "Epoch 42/1000 — loss: 1.2942 — acc: 0.6875\n",
      "Epoch 43/1000 — loss: 1.2721 — acc: 0.6875\n",
      "Epoch 44/1000 — loss: 1.1980 — acc: 0.6875\n",
      "Epoch 45/1000 — loss: 1.4035 — acc: 0.6875\n",
      "Epoch 46/1000 — loss: 1.3471 — acc: 0.6250\n",
      "Epoch 47/1000 — loss: 1.2403 — acc: 0.6250\n",
      "Epoch 48/1000 — loss: 1.1741 — acc: 0.6250\n",
      "Epoch 49/1000 — loss: 1.2715 — acc: 0.6250\n",
      "Epoch 50/1000 — loss: 1.2131 — acc: 0.5625\n",
      "Epoch 51/1000 — loss: 1.0137 — acc: 0.8750\n",
      "Epoch 52/1000 — loss: 1.0801 — acc: 0.8125\n",
      "Epoch 53/1000 — loss: 1.0519 — acc: 0.6250\n",
      "Epoch 54/1000 — loss: 0.9795 — acc: 0.7500\n",
      "Epoch 55/1000 — loss: 1.1755 — acc: 0.5625\n",
      "Epoch 56/1000 — loss: 1.0036 — acc: 0.6250\n",
      "Epoch 57/1000 — loss: 1.1989 — acc: 0.5625\n",
      "Epoch 58/1000 — loss: 0.9610 — acc: 0.6875\n",
      "Epoch 59/1000 — loss: 0.8705 — acc: 0.7500\n",
      "Epoch 60/1000 — loss: 1.1301 — acc: 0.6250\n",
      "Epoch 61/1000 — loss: 1.0706 — acc: 0.6875\n",
      "Epoch 62/1000 — loss: 0.9623 — acc: 0.7500\n",
      "Epoch 63/1000 — loss: 0.9284 — acc: 0.8125\n",
      "Epoch 64/1000 — loss: 1.0184 — acc: 0.7500\n",
      "Epoch 65/1000 — loss: 0.8742 — acc: 0.8125\n",
      "Epoch 66/1000 — loss: 0.9028 — acc: 0.7500\n",
      "Epoch 67/1000 — loss: 0.9301 — acc: 0.6250\n",
      "Epoch 68/1000 — loss: 0.8764 — acc: 0.6875\n",
      "Epoch 69/1000 — loss: 0.9180 — acc: 0.8750\n",
      "Epoch 70/1000 — loss: 1.1304 — acc: 0.6875\n",
      "Epoch 71/1000 — loss: 0.9116 — acc: 0.7500\n",
      "Epoch 72/1000 — loss: 0.9726 — acc: 0.8125\n",
      "Epoch 73/1000 — loss: 0.8549 — acc: 0.8750\n",
      "Epoch 74/1000 — loss: 0.9638 — acc: 0.7500\n",
      "Epoch 75/1000 — loss: 0.7487 — acc: 0.8750\n",
      "Epoch 76/1000 — loss: 0.8376 — acc: 0.8125\n",
      "Epoch 77/1000 — loss: 0.8598 — acc: 0.7500\n",
      "Epoch 78/1000 — loss: 0.8825 — acc: 0.7500\n",
      "Epoch 79/1000 — loss: 0.7532 — acc: 0.9375\n",
      "Epoch 80/1000 — loss: 0.8434 — acc: 0.7500\n",
      "Epoch 81/1000 — loss: 0.8997 — acc: 0.7500\n",
      "Epoch 82/1000 — loss: 0.8673 — acc: 0.7500\n",
      "Epoch 83/1000 — loss: 0.7392 — acc: 0.8125\n",
      "Epoch 84/1000 — loss: 0.7537 — acc: 0.8125\n",
      "Epoch 85/1000 — loss: 0.9086 — acc: 0.7500\n",
      "Epoch 86/1000 — loss: 0.7902 — acc: 0.6875\n",
      "Epoch 87/1000 — loss: 0.8950 — acc: 0.6875\n",
      "Epoch 88/1000 — loss: 0.6707 — acc: 0.8125\n",
      "Epoch 89/1000 — loss: 0.7415 — acc: 0.7500\n",
      "Epoch 90/1000 — loss: 0.6322 — acc: 0.8750\n",
      "Epoch 91/1000 — loss: 0.6597 — acc: 0.8750\n",
      "Epoch 92/1000 — loss: 0.7191 — acc: 0.8750\n",
      "Epoch 93/1000 — loss: 0.6321 — acc: 0.8750\n",
      "Epoch 94/1000 — loss: 0.7394 — acc: 0.6875\n",
      "Epoch 95/1000 — loss: 0.6094 — acc: 0.8750\n",
      "Epoch 96/1000 — loss: 0.6310 — acc: 0.8125\n",
      "Epoch 97/1000 — loss: 0.5626 — acc: 0.8750\n",
      "Epoch 98/1000 — loss: 0.6739 — acc: 0.8125\n",
      "Epoch 99/1000 — loss: 0.7185 — acc: 0.8125\n",
      "Epoch 100/1000 — loss: 0.5297 — acc: 0.9375\n",
      "Epoch 101/1000 — loss: 0.5929 — acc: 0.8750\n",
      "Epoch 102/1000 — loss: 0.5496 — acc: 0.8750\n",
      "Epoch 103/1000 — loss: 0.5053 — acc: 0.8750\n",
      "Epoch 104/1000 — loss: 0.5518 — acc: 0.8125\n",
      "Epoch 105/1000 — loss: 0.5851 — acc: 0.8125\n",
      "Epoch 106/1000 — loss: 0.6046 — acc: 0.8750\n",
      "Epoch 107/1000 — loss: 0.5753 — acc: 0.8750\n",
      "Epoch 108/1000 — loss: 0.6013 — acc: 0.8125\n",
      "Epoch 109/1000 — loss: 0.5222 — acc: 0.8125\n",
      "Epoch 110/1000 — loss: 0.5555 — acc: 0.8125\n",
      "Epoch 111/1000 — loss: 0.5415 — acc: 0.8750\n",
      "Epoch 112/1000 — loss: 0.5358 — acc: 0.8125\n",
      "Epoch 113/1000 — loss: 0.5448 — acc: 0.8125\n",
      "Epoch 114/1000 — loss: 0.6101 — acc: 0.7500\n",
      "Epoch 115/1000 — loss: 0.5865 — acc: 0.9375\n",
      "Epoch 116/1000 — loss: 0.5200 — acc: 0.8125\n",
      "Epoch 117/1000 — loss: 0.6478 — acc: 0.7500\n",
      "Epoch 118/1000 — loss: 0.4691 — acc: 0.8750\n",
      "Epoch 119/1000 — loss: 0.4452 — acc: 0.9375\n",
      "Epoch 120/1000 — loss: 0.6460 — acc: 0.7500\n",
      "Epoch 121/1000 — loss: 0.5445 — acc: 0.7500\n",
      "Epoch 122/1000 — loss: 0.5873 — acc: 0.8125\n",
      "Epoch 123/1000 — loss: 0.5889 — acc: 0.8125\n",
      "Epoch 124/1000 — loss: 0.5068 — acc: 0.8750\n",
      "Epoch 125/1000 — loss: 0.5302 — acc: 0.8125\n",
      "Epoch 126/1000 — loss: 0.4545 — acc: 0.8750\n",
      "Epoch 127/1000 — loss: 0.5077 — acc: 0.8125\n",
      "Epoch 128/1000 — loss: 0.5658 — acc: 0.8125\n",
      "Epoch 129/1000 — loss: 0.5139 — acc: 0.9375\n",
      "Epoch 130/1000 — loss: 0.5301 — acc: 0.8125\n",
      "Epoch 131/1000 — loss: 0.4086 — acc: 0.9375\n",
      "Epoch 132/1000 — loss: 0.5774 — acc: 0.8750\n",
      "Epoch 133/1000 — loss: 0.4782 — acc: 0.9375\n",
      "Epoch 134/1000 — loss: 0.5252 — acc: 0.8125\n",
      "Epoch 135/1000 — loss: 0.4363 — acc: 0.8750\n",
      "Epoch 136/1000 — loss: 0.4074 — acc: 0.9375\n",
      "Epoch 137/1000 — loss: 0.4675 — acc: 0.8125\n",
      "Epoch 138/1000 — loss: 0.5327 — acc: 0.8750\n",
      "Epoch 139/1000 — loss: 0.5001 — acc: 0.8750\n",
      "Epoch 140/1000 — loss: 0.4689 — acc: 0.8750\n",
      "Epoch 141/1000 — loss: 0.4869 — acc: 0.8125\n",
      "Epoch 142/1000 — loss: 0.4891 — acc: 0.8750\n",
      "Epoch 143/1000 — loss: 0.4838 — acc: 0.9375\n",
      "Epoch 144/1000 — loss: 0.4031 — acc: 0.8750\n",
      "Epoch 145/1000 — loss: 0.4894 — acc: 0.8125\n",
      "Epoch 146/1000 — loss: 0.4565 — acc: 0.8750\n",
      "Epoch 147/1000 — loss: 0.3754 — acc: 1.0000\n",
      "Epoch 148/1000 — loss: 0.3545 — acc: 1.0000\n",
      "Epoch 149/1000 — loss: 0.4042 — acc: 0.9375\n",
      "Epoch 150/1000 — loss: 0.4072 — acc: 0.8750\n",
      "Epoch 151/1000 — loss: 0.4198 — acc: 0.8125\n",
      "Epoch 152/1000 — loss: 0.3932 — acc: 0.9375\n",
      "Epoch 153/1000 — loss: 0.3975 — acc: 0.8125\n",
      "Epoch 154/1000 — loss: 0.4193 — acc: 0.8125\n",
      "Epoch 155/1000 — loss: 0.3279 — acc: 0.9375\n",
      "Epoch 156/1000 — loss: 0.4151 — acc: 0.8750\n",
      "Epoch 157/1000 — loss: 0.4415 — acc: 0.8125\n",
      "Epoch 158/1000 — loss: 0.4186 — acc: 0.9375\n",
      "Epoch 159/1000 — loss: 0.4491 — acc: 0.8750\n",
      "Epoch 160/1000 — loss: 0.3481 — acc: 0.9375\n",
      "Epoch 161/1000 — loss: 0.3699 — acc: 0.9375\n",
      "Epoch 162/1000 — loss: 0.4170 — acc: 0.8125\n",
      "Epoch 163/1000 — loss: 0.4268 — acc: 0.8125\n",
      "Epoch 164/1000 — loss: 0.3665 — acc: 0.8750\n",
      "Epoch 165/1000 — loss: 0.3464 — acc: 0.9375\n",
      "Epoch 166/1000 — loss: 0.5628 — acc: 0.8750\n",
      "Epoch 167/1000 — loss: 0.4046 — acc: 0.8750\n",
      "Epoch 168/1000 — loss: 0.3655 — acc: 0.8750\n",
      "Epoch 169/1000 — loss: 0.4386 — acc: 0.8750\n",
      "Epoch 170/1000 — loss: 0.3513 — acc: 0.8750\n",
      "Epoch 171/1000 — loss: 0.3501 — acc: 0.9375\n",
      "Epoch 172/1000 — loss: 0.3840 — acc: 0.8125\n",
      "Epoch 173/1000 — loss: 0.4089 — acc: 0.9375\n",
      "Epoch 174/1000 — loss: 0.3944 — acc: 0.8125\n",
      "Epoch 175/1000 — loss: 0.3234 — acc: 1.0000\n",
      "Epoch 176/1000 — loss: 0.3903 — acc: 0.8125\n",
      "Epoch 177/1000 — loss: 0.5697 — acc: 0.7500\n",
      "Epoch 178/1000 — loss: 0.4973 — acc: 0.7500\n",
      "Epoch 179/1000 — loss: 0.4010 — acc: 0.8750\n",
      "Epoch 180/1000 — loss: 0.3514 — acc: 0.9375\n",
      "Epoch 181/1000 — loss: 0.3691 — acc: 0.8750\n",
      "Epoch 182/1000 — loss: 0.3197 — acc: 0.8125\n",
      "Epoch 183/1000 — loss: 0.3366 — acc: 0.8125\n",
      "Epoch 184/1000 — loss: 0.3270 — acc: 0.8125\n",
      "Epoch 185/1000 — loss: 0.3324 — acc: 0.8750\n",
      "Epoch 186/1000 — loss: 0.3174 — acc: 0.8125\n",
      "Epoch 187/1000 — loss: 0.4129 — acc: 0.8125\n",
      "Epoch 188/1000 — loss: 0.3401 — acc: 0.9375\n",
      "Epoch 189/1000 — loss: 0.3598 — acc: 0.8125\n",
      "Epoch 190/1000 — loss: 0.3319 — acc: 0.8750\n",
      "Epoch 191/1000 — loss: 0.3118 — acc: 0.8750\n",
      "Epoch 192/1000 — loss: 0.3341 — acc: 0.9375\n",
      "Epoch 193/1000 — loss: 0.3010 — acc: 0.8750\n",
      "Epoch 194/1000 — loss: 0.3497 — acc: 0.9375\n",
      "Epoch 195/1000 — loss: 0.3643 — acc: 0.8125\n",
      "Epoch 196/1000 — loss: 0.3922 — acc: 0.8125\n",
      "Epoch 197/1000 — loss: 0.2811 — acc: 0.8750\n",
      "Epoch 198/1000 — loss: 0.3393 — acc: 0.8125\n",
      "Epoch 199/1000 — loss: 0.3388 — acc: 0.8750\n",
      "Epoch 200/1000 — loss: 0.3042 — acc: 0.8125\n",
      "Epoch 201/1000 — loss: 0.3427 — acc: 0.8750\n",
      "Epoch 202/1000 — loss: 0.2592 — acc: 0.9375\n",
      "Epoch 203/1000 — loss: 0.3333 — acc: 0.8750\n",
      "Epoch 204/1000 — loss: 0.3117 — acc: 0.8750\n",
      "Epoch 205/1000 — loss: 0.3158 — acc: 0.8750\n",
      "Epoch 206/1000 — loss: 0.3304 — acc: 0.8750\n",
      "Epoch 207/1000 — loss: 0.3471 — acc: 0.8125\n",
      "Epoch 208/1000 — loss: 0.2981 — acc: 0.9375\n",
      "Epoch 209/1000 — loss: 0.2715 — acc: 0.9375\n",
      "Epoch 210/1000 — loss: 0.2933 — acc: 0.8750\n",
      "Epoch 211/1000 — loss: 0.3011 — acc: 0.8125\n",
      "Epoch 212/1000 — loss: 0.2921 — acc: 0.8750\n",
      "Epoch 213/1000 — loss: 0.2898 — acc: 0.8750\n",
      "Epoch 214/1000 — loss: 0.2995 — acc: 0.9375\n",
      "Epoch 215/1000 — loss: 0.3563 — acc: 0.8750\n",
      "Epoch 216/1000 — loss: 0.2782 — acc: 0.8750\n",
      "Epoch 217/1000 — loss: 0.3047 — acc: 0.8125\n",
      "Epoch 218/1000 — loss: 0.2535 — acc: 0.8750\n",
      "Epoch 219/1000 — loss: 0.3184 — acc: 0.8750\n",
      "Epoch 220/1000 — loss: 0.2850 — acc: 0.9375\n",
      "Epoch 221/1000 — loss: 0.2689 — acc: 0.8750\n",
      "Epoch 222/1000 — loss: 0.2375 — acc: 0.9375\n",
      "Epoch 223/1000 — loss: 0.4013 — acc: 0.8125\n",
      "Epoch 224/1000 — loss: 0.2710 — acc: 0.8750\n",
      "Epoch 225/1000 — loss: 0.3403 — acc: 0.8750\n",
      "Epoch 226/1000 — loss: 0.2710 — acc: 0.9375\n",
      "Epoch 227/1000 — loss: 0.3065 — acc: 0.8125\n",
      "Epoch 228/1000 — loss: 0.2830 — acc: 0.8125\n",
      "Epoch 229/1000 — loss: 0.2790 — acc: 0.8750\n",
      "Epoch 230/1000 — loss: 0.3129 — acc: 0.8750\n",
      "Epoch 231/1000 — loss: 0.3019 — acc: 0.9375\n",
      "Epoch 232/1000 — loss: 0.3252 — acc: 0.8750\n",
      "Epoch 233/1000 — loss: 0.3121 — acc: 0.8750\n",
      "Epoch 234/1000 — loss: 0.2790 — acc: 0.8750\n",
      "Epoch 235/1000 — loss: 0.4690 — acc: 0.7500\n",
      "Epoch 236/1000 — loss: 0.2429 — acc: 1.0000\n",
      "Epoch 237/1000 — loss: 0.2940 — acc: 0.8750\n",
      "Epoch 238/1000 — loss: 0.2941 — acc: 0.8125\n",
      "Epoch 239/1000 — loss: 0.3161 — acc: 0.8125\n",
      "Epoch 240/1000 — loss: 0.2783 — acc: 0.9375\n",
      "Epoch 241/1000 — loss: 0.2647 — acc: 0.8750\n",
      "Epoch 242/1000 — loss: 0.2881 — acc: 0.8750\n",
      "Epoch 243/1000 — loss: 0.3128 — acc: 0.8125\n",
      "Epoch 244/1000 — loss: 0.2721 — acc: 0.9375\n",
      "Epoch 245/1000 — loss: 0.2874 — acc: 0.8750\n",
      "Epoch 246/1000 — loss: 0.3462 — acc: 0.8750\n",
      "Epoch 247/1000 — loss: 0.2800 — acc: 0.8125\n",
      "Epoch 248/1000 — loss: 0.3201 — acc: 0.8125\n",
      "Epoch 249/1000 — loss: 0.2666 — acc: 0.9375\n",
      "Epoch 250/1000 — loss: 0.2614 — acc: 0.8750\n",
      "Epoch 251/1000 — loss: 0.2772 — acc: 0.8750\n",
      "Epoch 252/1000 — loss: 0.2855 — acc: 0.8750\n",
      "Epoch 253/1000 — loss: 0.2918 — acc: 0.8750\n",
      "Epoch 254/1000 — loss: 0.2702 — acc: 0.8750\n",
      "Epoch 255/1000 — loss: 0.3013 — acc: 0.8125\n",
      "Epoch 256/1000 — loss: 0.2472 — acc: 0.9375\n",
      "Epoch 257/1000 — loss: 0.2908 — acc: 0.8750\n",
      "Epoch 258/1000 — loss: 0.3368 — acc: 0.8125\n",
      "Epoch 259/1000 — loss: 0.2951 — acc: 0.8750\n",
      "Epoch 260/1000 — loss: 0.3019 — acc: 0.8125\n",
      "Epoch 261/1000 — loss: 0.2165 — acc: 0.9375\n",
      "Epoch 262/1000 — loss: 0.2936 — acc: 0.8125\n",
      "Epoch 263/1000 — loss: 0.2507 — acc: 0.8125\n",
      "Epoch 264/1000 — loss: 0.2724 — acc: 1.0000\n",
      "Epoch 265/1000 — loss: 0.2672 — acc: 0.8125\n",
      "Epoch 266/1000 — loss: 0.3293 — acc: 0.8125\n",
      "Epoch 267/1000 — loss: 0.2879 — acc: 0.9375\n",
      "Epoch 268/1000 — loss: 0.2758 — acc: 0.8125\n",
      "Epoch 269/1000 — loss: 0.2457 — acc: 0.8750\n",
      "Epoch 270/1000 — loss: 0.2632 — acc: 0.8750\n",
      "Epoch 271/1000 — loss: 0.2630 — acc: 0.8750\n",
      "Epoch 272/1000 — loss: 0.2465 — acc: 0.8750\n",
      "Epoch 273/1000 — loss: 0.2772 — acc: 0.8125\n",
      "Epoch 274/1000 — loss: 0.2804 — acc: 0.8125\n",
      "Epoch 275/1000 — loss: 0.2846 — acc: 0.8750\n",
      "Epoch 276/1000 — loss: 0.2662 — acc: 0.8750\n",
      "Epoch 277/1000 — loss: 0.2507 — acc: 0.8125\n",
      "Epoch 278/1000 — loss: 0.2732 — acc: 0.8750\n",
      "Epoch 279/1000 — loss: 0.2571 — acc: 0.8125\n",
      "Epoch 280/1000 — loss: 0.2839 — acc: 0.8750\n",
      "Epoch 281/1000 — loss: 0.2609 — acc: 0.9375\n",
      "Epoch 282/1000 — loss: 0.3065 — acc: 0.8125\n",
      "Epoch 283/1000 — loss: 0.2600 — acc: 0.8750\n",
      "Epoch 284/1000 — loss: 0.2862 — acc: 0.8125\n",
      "Epoch 285/1000 — loss: 0.2432 — acc: 0.9375\n",
      "Epoch 286/1000 — loss: 0.2465 — acc: 0.8750\n",
      "Epoch 287/1000 — loss: 0.3023 — acc: 0.8750\n",
      "Epoch 288/1000 — loss: 0.2632 — acc: 0.8125\n",
      "Epoch 289/1000 — loss: 0.2628 — acc: 0.8750\n",
      "Epoch 290/1000 — loss: 0.2936 — acc: 0.8125\n",
      "Epoch 291/1000 — loss: 0.2658 — acc: 0.8750\n",
      "Epoch 292/1000 — loss: 0.2959 — acc: 0.8125\n",
      "Epoch 293/1000 — loss: 0.2815 — acc: 0.9375\n",
      "Epoch 294/1000 — loss: 0.2544 — acc: 0.8750\n",
      "Epoch 295/1000 — loss: 0.2565 — acc: 0.9375\n",
      "Epoch 296/1000 — loss: 0.2905 — acc: 0.8125\n",
      "Epoch 297/1000 — loss: 0.2770 — acc: 0.9375\n",
      "Epoch 298/1000 — loss: 0.2639 — acc: 0.8125\n",
      "Epoch 299/1000 — loss: 0.2503 — acc: 1.0000\n",
      "Epoch 300/1000 — loss: 0.2498 — acc: 0.8750\n",
      "Epoch 301/1000 — loss: 0.2319 — acc: 0.9375\n",
      "Epoch 302/1000 — loss: 0.2351 — acc: 0.8750\n",
      "Epoch 303/1000 — loss: 0.2603 — acc: 0.8750\n",
      "Epoch 304/1000 — loss: 0.2452 — acc: 0.8750\n",
      "Epoch 305/1000 — loss: 0.2565 — acc: 0.9375\n",
      "Epoch 306/1000 — loss: 0.2429 — acc: 0.8750\n",
      "Epoch 307/1000 — loss: 0.2567 — acc: 0.8750\n",
      "Epoch 308/1000 — loss: 0.2602 — acc: 0.8750\n",
      "Epoch 309/1000 — loss: 0.2551 — acc: 0.8125\n",
      "Epoch 310/1000 — loss: 0.2614 — acc: 0.8125\n",
      "Epoch 311/1000 — loss: 0.2769 — acc: 0.8750\n",
      "Epoch 312/1000 — loss: 0.2641 — acc: 0.8750\n",
      "Epoch 313/1000 — loss: 0.2835 — acc: 0.8125\n",
      "Epoch 314/1000 — loss: 0.2314 — acc: 0.8750\n",
      "Epoch 315/1000 — loss: 0.2554 — acc: 0.8125\n",
      "Epoch 316/1000 — loss: 0.2763 — acc: 0.8750\n",
      "Epoch 317/1000 — loss: 0.2668 — acc: 0.8750\n",
      "Epoch 318/1000 — loss: 0.2661 — acc: 0.8750\n",
      "Epoch 319/1000 — loss: 0.3026 — acc: 0.8125\n",
      "Epoch 320/1000 — loss: 0.2680 — acc: 0.8125\n",
      "Epoch 321/1000 — loss: 0.2368 — acc: 0.8750\n",
      "Epoch 322/1000 — loss: 0.2489 — acc: 0.8750\n",
      "Epoch 323/1000 — loss: 0.2624 — acc: 0.8750\n",
      "Epoch 324/1000 — loss: 0.2777 — acc: 0.8750\n",
      "Epoch 325/1000 — loss: 0.2191 — acc: 0.8750\n",
      "Epoch 326/1000 — loss: 0.1879 — acc: 0.9375\n",
      "Epoch 327/1000 — loss: 0.2092 — acc: 0.9375\n",
      "Epoch 328/1000 — loss: 0.3249 — acc: 0.8125\n",
      "Epoch 329/1000 — loss: 0.2460 — acc: 0.8750\n",
      "Epoch 330/1000 — loss: 0.2274 — acc: 0.9375\n",
      "Epoch 331/1000 — loss: 0.2630 — acc: 0.9375\n",
      "Epoch 332/1000 — loss: 0.2724 — acc: 0.8125\n",
      "Epoch 333/1000 — loss: 0.2472 — acc: 0.8750\n",
      "Epoch 334/1000 — loss: 0.2392 — acc: 0.8750\n",
      "Epoch 335/1000 — loss: 0.2445 — acc: 0.9375\n",
      "Epoch 336/1000 — loss: 0.2415 — acc: 0.8125\n",
      "Epoch 337/1000 — loss: 0.2305 — acc: 0.9375\n",
      "Epoch 338/1000 — loss: 0.2611 — acc: 0.8125\n",
      "Epoch 339/1000 — loss: 0.2740 — acc: 0.8125\n",
      "Epoch 340/1000 — loss: 0.2493 — acc: 0.9375\n",
      "Epoch 341/1000 — loss: 0.2161 — acc: 0.9375\n",
      "Epoch 342/1000 — loss: 0.2000 — acc: 1.0000\n",
      "Epoch 343/1000 — loss: 0.2528 — acc: 0.8125\n",
      "Epoch 344/1000 — loss: 0.2474 — acc: 0.8750\n",
      "Epoch 345/1000 — loss: 0.2434 — acc: 0.8125\n",
      "Epoch 346/1000 — loss: 0.2516 — acc: 0.8750\n",
      "Epoch 347/1000 — loss: 0.2825 — acc: 0.8125\n",
      "Epoch 348/1000 — loss: 0.2301 — acc: 1.0000\n",
      "Epoch 349/1000 — loss: 0.2476 — acc: 0.9375\n",
      "Epoch 350/1000 — loss: 0.2346 — acc: 0.8750\n",
      "Epoch 351/1000 — loss: 0.2129 — acc: 1.0000\n",
      "Epoch 352/1000 — loss: 0.3030 — acc: 0.8125\n",
      "Epoch 353/1000 — loss: 0.2333 — acc: 0.8750\n",
      "Epoch 354/1000 — loss: 0.2503 — acc: 0.8125\n",
      "Epoch 355/1000 — loss: 0.2395 — acc: 0.9375\n",
      "Epoch 356/1000 — loss: 0.2681 — acc: 0.9375\n",
      "Epoch 357/1000 — loss: 0.2849 — acc: 0.8125\n",
      "Epoch 358/1000 — loss: 0.2500 — acc: 0.8750\n",
      "Epoch 359/1000 — loss: 0.2711 — acc: 0.8750\n",
      "Epoch 360/1000 — loss: 0.2122 — acc: 1.0000\n",
      "Epoch 361/1000 — loss: 0.2482 — acc: 0.8750\n",
      "Epoch 362/1000 — loss: 0.2421 — acc: 0.8125\n",
      "Epoch 363/1000 — loss: 0.2091 — acc: 1.0000\n",
      "Epoch 364/1000 — loss: 0.2311 — acc: 0.8750\n",
      "Epoch 365/1000 — loss: 0.2766 — acc: 0.8125\n",
      "Epoch 366/1000 — loss: 0.2337 — acc: 0.8750\n",
      "Epoch 367/1000 — loss: 0.2081 — acc: 1.0000\n",
      "Epoch 368/1000 — loss: 0.2248 — acc: 0.8750\n",
      "Epoch 369/1000 — loss: 0.2353 — acc: 0.8750\n",
      "Epoch 370/1000 — loss: 0.2203 — acc: 0.8750\n",
      "Epoch 371/1000 — loss: 0.2220 — acc: 0.9375\n",
      "Epoch 372/1000 — loss: 0.2299 — acc: 0.8750\n",
      "Epoch 373/1000 — loss: 0.2478 — acc: 0.8750\n",
      "Epoch 374/1000 — loss: 0.2506 — acc: 0.8750\n",
      "Epoch 375/1000 — loss: 0.2268 — acc: 0.9375\n",
      "Epoch 376/1000 — loss: 0.2004 — acc: 1.0000\n",
      "Epoch 377/1000 — loss: 0.2191 — acc: 1.0000\n",
      "Epoch 378/1000 — loss: 0.2695 — acc: 0.8125\n",
      "Epoch 379/1000 — loss: 0.2214 — acc: 0.8750\n",
      "Epoch 380/1000 — loss: 0.2700 — acc: 0.8750\n",
      "Epoch 381/1000 — loss: 0.1902 — acc: 1.0000\n",
      "Epoch 382/1000 — loss: 0.2720 — acc: 0.8125\n",
      "Epoch 383/1000 — loss: 0.2506 — acc: 0.8750\n",
      "Epoch 384/1000 — loss: 0.2714 — acc: 0.8125\n",
      "Epoch 385/1000 — loss: 0.2292 — acc: 0.9375\n",
      "Epoch 386/1000 — loss: 0.2739 — acc: 0.8125\n",
      "Epoch 387/1000 — loss: 0.2443 — acc: 0.8750\n",
      "Epoch 388/1000 — loss: 0.2247 — acc: 0.9375\n",
      "Epoch 389/1000 — loss: 0.2330 — acc: 0.8750\n",
      "Epoch 390/1000 — loss: 0.2394 — acc: 0.8750\n",
      "Epoch 391/1000 — loss: 0.2509 — acc: 0.8125\n",
      "Epoch 392/1000 — loss: 0.2500 — acc: 0.8750\n",
      "Epoch 393/1000 — loss: 0.2281 — acc: 0.8750\n",
      "Epoch 394/1000 — loss: 0.2771 — acc: 0.8125\n",
      "Epoch 395/1000 — loss: 0.2423 — acc: 0.8750\n",
      "Epoch 396/1000 — loss: 0.2618 — acc: 0.8125\n",
      "Epoch 397/1000 — loss: 0.2309 — acc: 0.8750\n",
      "Epoch 398/1000 — loss: 0.2293 — acc: 0.8750\n",
      "Epoch 399/1000 — loss: 0.2405 — acc: 0.9375\n",
      "Epoch 400/1000 — loss: 0.2167 — acc: 0.8750\n",
      "Epoch 401/1000 — loss: 0.2165 — acc: 0.9375\n",
      "Epoch 402/1000 — loss: 0.2241 — acc: 0.9375\n",
      "Epoch 403/1000 — loss: 0.2323 — acc: 0.8750\n",
      "Epoch 404/1000 — loss: 0.2704 — acc: 0.8125\n",
      "Epoch 405/1000 — loss: 0.2595 — acc: 0.8750\n",
      "Epoch 406/1000 — loss: 0.2824 — acc: 0.8750\n",
      "Epoch 407/1000 — loss: 0.2401 — acc: 0.9375\n",
      "Epoch 408/1000 — loss: 0.2596 — acc: 0.8125\n",
      "Epoch 409/1000 — loss: 0.2574 — acc: 0.8750\n",
      "Epoch 410/1000 — loss: 0.2180 — acc: 0.8750\n",
      "Epoch 411/1000 — loss: 0.2310 — acc: 0.8125\n",
      "Epoch 412/1000 — loss: 0.2310 — acc: 0.8750\n",
      "Epoch 413/1000 — loss: 0.2644 — acc: 0.8125\n",
      "Epoch 414/1000 — loss: 0.2012 — acc: 0.9375\n",
      "Epoch 415/1000 — loss: 0.2169 — acc: 0.9375\n",
      "Epoch 416/1000 — loss: 0.2645 — acc: 0.8125\n",
      "Epoch 417/1000 — loss: 0.2836 — acc: 0.8125\n",
      "Epoch 418/1000 — loss: 0.2176 — acc: 0.9375\n",
      "Epoch 419/1000 — loss: 0.2285 — acc: 0.8750\n",
      "Epoch 420/1000 — loss: 0.2547 — acc: 0.8750\n",
      "Epoch 421/1000 — loss: 0.2447 — acc: 0.8125\n",
      "Epoch 422/1000 — loss: 0.2579 — acc: 0.8750\n",
      "Epoch 423/1000 — loss: 0.2283 — acc: 0.9375\n",
      "Epoch 424/1000 — loss: 0.2860 — acc: 0.8125\n",
      "Epoch 425/1000 — loss: 0.2842 — acc: 0.8125\n",
      "Epoch 426/1000 — loss: 0.2168 — acc: 0.8750\n",
      "Epoch 427/1000 — loss: 0.2488 — acc: 0.8125\n",
      "Epoch 428/1000 — loss: 0.2407 — acc: 0.8750\n",
      "Epoch 429/1000 — loss: 0.2240 — acc: 0.9375\n",
      "Epoch 430/1000 — loss: 0.2378 — acc: 0.8750\n",
      "Epoch 431/1000 — loss: 0.2484 — acc: 0.8125\n",
      "Epoch 432/1000 — loss: 0.3165 — acc: 0.8125\n",
      "Epoch 433/1000 — loss: 0.2757 — acc: 0.8125\n",
      "Epoch 434/1000 — loss: 0.2390 — acc: 0.8125\n",
      "Epoch 435/1000 — loss: 0.2234 — acc: 0.8750\n",
      "Epoch 436/1000 — loss: 0.2342 — acc: 0.8750\n",
      "Epoch 437/1000 — loss: 0.2083 — acc: 1.0000\n",
      "Epoch 438/1000 — loss: 0.2267 — acc: 0.9375\n",
      "Epoch 439/1000 — loss: 0.2120 — acc: 0.8750\n",
      "Epoch 440/1000 — loss: 0.2242 — acc: 0.8750\n",
      "Epoch 441/1000 — loss: 0.3094 — acc: 0.8125\n",
      "Epoch 442/1000 — loss: 0.2301 — acc: 0.8750\n",
      "Epoch 443/1000 — loss: 0.2377 — acc: 0.8125\n",
      "Epoch 444/1000 — loss: 0.2303 — acc: 0.8750\n",
      "Epoch 445/1000 — loss: 0.2188 — acc: 0.8750\n",
      "Epoch 446/1000 — loss: 0.2171 — acc: 0.9375\n",
      "Epoch 447/1000 — loss: 0.2250 — acc: 0.8125\n",
      "Epoch 448/1000 — loss: 0.2189 — acc: 0.9375\n",
      "Epoch 449/1000 — loss: 0.2412 — acc: 0.9375\n",
      "Epoch 450/1000 — loss: 0.2005 — acc: 0.8750\n",
      "Epoch 451/1000 — loss: 0.2358 — acc: 0.8750\n",
      "Epoch 452/1000 — loss: 0.2414 — acc: 0.9375\n",
      "Epoch 453/1000 — loss: 0.2495 — acc: 0.8125\n",
      "Epoch 454/1000 — loss: 0.2304 — acc: 0.8125\n",
      "Epoch 455/1000 — loss: 0.2106 — acc: 0.9375\n",
      "Epoch 456/1000 — loss: 0.2253 — acc: 0.8750\n",
      "Epoch 457/1000 — loss: 0.2573 — acc: 0.8125\n",
      "Epoch 458/1000 — loss: 0.1902 — acc: 1.0000\n",
      "Epoch 459/1000 — loss: 0.2442 — acc: 0.8750\n",
      "Epoch 460/1000 — loss: 0.2472 — acc: 0.8750\n",
      "Epoch 461/1000 — loss: 0.2352 — acc: 0.8750\n",
      "Epoch 462/1000 — loss: 0.2589 — acc: 0.8125\n",
      "Epoch 463/1000 — loss: 0.2028 — acc: 0.9375\n",
      "Epoch 464/1000 — loss: 0.2297 — acc: 0.8750\n",
      "Epoch 465/1000 — loss: 0.2147 — acc: 0.9375\n",
      "Epoch 466/1000 — loss: 0.2013 — acc: 0.9375\n",
      "Epoch 467/1000 — loss: 0.2666 — acc: 0.8125\n",
      "Epoch 468/1000 — loss: 0.2451 — acc: 0.8125\n",
      "Epoch 469/1000 — loss: 0.2844 — acc: 0.8125\n",
      "Epoch 470/1000 — loss: 0.2314 — acc: 0.8750\n",
      "Epoch 471/1000 — loss: 0.2115 — acc: 0.9375\n",
      "Epoch 472/1000 — loss: 0.2078 — acc: 0.8750\n",
      "Epoch 473/1000 — loss: 0.2509 — acc: 0.8125\n",
      "Epoch 474/1000 — loss: 0.2614 — acc: 0.8125\n",
      "Epoch 475/1000 — loss: 0.2739 — acc: 0.8125\n",
      "Epoch 476/1000 — loss: 0.2368 — acc: 0.8750\n",
      "Epoch 477/1000 — loss: 0.2234 — acc: 0.9375\n",
      "Epoch 478/1000 — loss: 0.2294 — acc: 0.8750\n",
      "Epoch 479/1000 — loss: 0.2229 — acc: 0.8750\n",
      "Epoch 480/1000 — loss: 0.2343 — acc: 0.9375\n",
      "Epoch 481/1000 — loss: 0.2163 — acc: 1.0000\n",
      "Epoch 482/1000 — loss: 0.2338 — acc: 0.8750\n",
      "Epoch 483/1000 — loss: 0.2453 — acc: 0.8125\n",
      "Epoch 484/1000 — loss: 0.2263 — acc: 0.8750\n",
      "Epoch 485/1000 — loss: 0.2211 — acc: 0.8750\n",
      "Epoch 486/1000 — loss: 0.1858 — acc: 0.9375\n",
      "Epoch 487/1000 — loss: 0.2775 — acc: 0.8750\n",
      "Epoch 488/1000 — loss: 0.1879 — acc: 0.9375\n",
      "Epoch 489/1000 — loss: 0.1778 — acc: 1.0000\n",
      "Epoch 490/1000 — loss: 0.3822 — acc: 0.7500\n",
      "Epoch 491/1000 — loss: 0.2305 — acc: 0.8750\n",
      "Epoch 492/1000 — loss: 0.2703 — acc: 0.8125\n",
      "Epoch 493/1000 — loss: 0.2274 — acc: 0.8750\n",
      "Epoch 494/1000 — loss: 0.1887 — acc: 0.9375\n",
      "Epoch 495/1000 — loss: 0.2504 — acc: 0.8125\n",
      "Epoch 496/1000 — loss: 0.2424 — acc: 0.8750\n",
      "Epoch 497/1000 — loss: 0.2168 — acc: 0.8750\n",
      "Epoch 498/1000 — loss: 0.4107 — acc: 0.7500\n",
      "Epoch 499/1000 — loss: 0.2367 — acc: 0.8125\n",
      "Epoch 500/1000 — loss: 0.1998 — acc: 0.9375\n",
      "Epoch 501/1000 — loss: 0.2334 — acc: 0.8750\n",
      "Epoch 502/1000 — loss: 0.2367 — acc: 0.8750\n",
      "Epoch 503/1000 — loss: 0.2685 — acc: 0.8750\n",
      "Epoch 504/1000 — loss: 0.2366 — acc: 0.8750\n",
      "Epoch 505/1000 — loss: 0.2403 — acc: 0.8750\n",
      "Epoch 506/1000 — loss: 0.2119 — acc: 0.9375\n",
      "Epoch 507/1000 — loss: 0.2619 — acc: 0.8125\n",
      "Epoch 508/1000 — loss: 0.2535 — acc: 0.8125\n",
      "Epoch 509/1000 — loss: 0.2308 — acc: 0.8750\n",
      "Epoch 510/1000 — loss: 0.2251 — acc: 0.8750\n",
      "Epoch 511/1000 — loss: 0.2378 — acc: 0.8125\n",
      "Epoch 512/1000 — loss: 0.2289 — acc: 0.8750\n",
      "Epoch 513/1000 — loss: 0.2400 — acc: 0.8125\n",
      "Epoch 514/1000 — loss: 0.2416 — acc: 0.8750\n",
      "Epoch 515/1000 — loss: 0.2568 — acc: 0.8125\n",
      "Epoch 516/1000 — loss: 0.2308 — acc: 0.8125\n",
      "Epoch 517/1000 — loss: 0.2065 — acc: 0.8750\n",
      "Epoch 518/1000 — loss: 0.2369 — acc: 0.8750\n",
      "Epoch 519/1000 — loss: 0.2089 — acc: 0.9375\n",
      "Epoch 520/1000 — loss: 0.2074 — acc: 0.8750\n",
      "Epoch 521/1000 — loss: 0.2269 — acc: 0.8750\n",
      "Epoch 522/1000 — loss: 0.2330 — acc: 0.8750\n",
      "Epoch 523/1000 — loss: 0.2267 — acc: 0.8750\n",
      "Epoch 524/1000 — loss: 0.2047 — acc: 0.8750\n",
      "Epoch 525/1000 — loss: 0.2619 — acc: 0.8125\n",
      "Epoch 526/1000 — loss: 0.2118 — acc: 0.8750\n",
      "Epoch 527/1000 — loss: 0.2247 — acc: 0.9375\n",
      "Epoch 528/1000 — loss: 0.1775 — acc: 1.0000\n",
      "Epoch 529/1000 — loss: 0.2262 — acc: 0.8750\n",
      "Epoch 530/1000 — loss: 0.2034 — acc: 0.9375\n",
      "Epoch 531/1000 — loss: 0.3273 — acc: 0.7500\n",
      "Epoch 532/1000 — loss: 0.2265 — acc: 0.8125\n",
      "Epoch 533/1000 — loss: 0.2247 — acc: 0.8750\n",
      "Epoch 534/1000 — loss: 0.2214 — acc: 0.9375\n",
      "Epoch 535/1000 — loss: 0.2323 — acc: 0.8750\n",
      "Epoch 536/1000 — loss: 0.2259 — acc: 0.9375\n",
      "Epoch 537/1000 — loss: 0.2237 — acc: 0.8750\n",
      "Epoch 538/1000 — loss: 0.2524 — acc: 0.8125\n",
      "Epoch 539/1000 — loss: 0.2019 — acc: 0.9375\n",
      "Epoch 540/1000 — loss: 0.2315 — acc: 0.8750\n",
      "Epoch 541/1000 — loss: 0.2237 — acc: 0.8750\n",
      "Epoch 542/1000 — loss: 0.1889 — acc: 1.0000\n",
      "Epoch 543/1000 — loss: 0.1622 — acc: 1.0000\n",
      "Epoch 544/1000 — loss: 0.2307 — acc: 0.8750\n",
      "Epoch 545/1000 — loss: 0.2476 — acc: 0.8750\n",
      "Epoch 546/1000 — loss: 0.2544 — acc: 0.8125\n",
      "Epoch 547/1000 — loss: 0.2724 — acc: 0.8125\n",
      "Epoch 548/1000 — loss: 0.2078 — acc: 0.9375\n",
      "Epoch 549/1000 — loss: 0.2479 — acc: 0.8125\n",
      "Epoch 550/1000 — loss: 0.2479 — acc: 0.8750\n",
      "Epoch 551/1000 — loss: 0.2302 — acc: 0.8750\n",
      "Epoch 552/1000 — loss: 0.2200 — acc: 0.8750\n",
      "Epoch 553/1000 — loss: 0.2285 — acc: 0.8750\n",
      "Epoch 554/1000 — loss: 0.2220 — acc: 0.8750\n",
      "Epoch 555/1000 — loss: 0.2268 — acc: 0.8125\n",
      "Epoch 556/1000 — loss: 0.2488 — acc: 0.9375\n",
      "Epoch 557/1000 — loss: 0.2335 — acc: 0.8125\n",
      "Epoch 558/1000 — loss: 0.1981 — acc: 0.9375\n",
      "Epoch 559/1000 — loss: 0.2012 — acc: 0.9375\n",
      "Epoch 560/1000 — loss: 0.2321 — acc: 0.8125\n",
      "Epoch 561/1000 — loss: 0.2057 — acc: 0.9375\n",
      "Epoch 562/1000 — loss: 0.2191 — acc: 0.8750\n",
      "Epoch 563/1000 — loss: 0.2109 — acc: 0.8125\n",
      "Epoch 564/1000 — loss: 0.2240 — acc: 0.8125\n",
      "Epoch 565/1000 — loss: 0.2478 — acc: 0.9375\n",
      "Epoch 566/1000 — loss: 0.2123 — acc: 0.8750\n",
      "Epoch 567/1000 — loss: 0.1934 — acc: 0.9375\n",
      "Epoch 568/1000 — loss: 0.2347 — acc: 0.8125\n",
      "Epoch 569/1000 — loss: 0.2358 — acc: 0.8750\n",
      "Epoch 570/1000 — loss: 0.1942 — acc: 1.0000\n",
      "Epoch 571/1000 — loss: 0.2315 — acc: 0.8750\n",
      "Epoch 572/1000 — loss: 0.2339 — acc: 0.8125\n",
      "Epoch 573/1000 — loss: 0.2412 — acc: 0.8125\n",
      "Epoch 574/1000 — loss: 0.2317 — acc: 0.8125\n",
      "Epoch 575/1000 — loss: 0.2284 — acc: 0.8750\n",
      "Epoch 576/1000 — loss: 0.2272 — acc: 0.8125\n",
      "Epoch 577/1000 — loss: 0.2231 — acc: 0.9375\n",
      "Epoch 578/1000 — loss: 0.2356 — acc: 0.8125\n",
      "Epoch 579/1000 — loss: 0.1904 — acc: 1.0000\n",
      "Epoch 580/1000 — loss: 0.2331 — acc: 0.8750\n",
      "Epoch 581/1000 — loss: 0.1988 — acc: 0.9375\n",
      "Epoch 582/1000 — loss: 0.2456 — acc: 0.8750\n",
      "Epoch 583/1000 — loss: 0.2018 — acc: 1.0000\n",
      "Epoch 584/1000 — loss: 0.2271 — acc: 0.8750\n",
      "Epoch 585/1000 — loss: 0.2613 — acc: 0.8750\n",
      "Epoch 586/1000 — loss: 0.2380 — acc: 0.8750\n",
      "Epoch 587/1000 — loss: 0.2133 — acc: 0.8750\n",
      "Epoch 588/1000 — loss: 0.2214 — acc: 0.9375\n",
      "Epoch 589/1000 — loss: 0.2640 — acc: 0.8125\n",
      "Epoch 590/1000 — loss: 0.2397 — acc: 0.8750\n",
      "Epoch 591/1000 — loss: 0.2211 — acc: 0.9375\n",
      "Epoch 592/1000 — loss: 0.2060 — acc: 0.9375\n",
      "Epoch 593/1000 — loss: 0.2447 — acc: 0.8125\n",
      "Epoch 594/1000 — loss: 0.2031 — acc: 0.9375\n",
      "Epoch 595/1000 — loss: 0.2079 — acc: 0.9375\n",
      "Epoch 596/1000 — loss: 0.2028 — acc: 0.9375\n",
      "Epoch 597/1000 — loss: 0.2276 — acc: 0.8125\n",
      "Epoch 598/1000 — loss: 0.2026 — acc: 0.9375\n",
      "Epoch 599/1000 — loss: 0.2245 — acc: 0.8125\n",
      "Epoch 600/1000 — loss: 0.2136 — acc: 0.8750\n",
      "Epoch 601/1000 — loss: 0.2132 — acc: 0.8750\n",
      "Epoch 602/1000 — loss: 0.2201 — acc: 0.9375\n",
      "Epoch 603/1000 — loss: 0.2445 — acc: 0.8125\n",
      "Epoch 604/1000 — loss: 0.2169 — acc: 0.9375\n",
      "Epoch 605/1000 — loss: 0.2135 — acc: 0.8750\n",
      "Epoch 606/1000 — loss: 0.2071 — acc: 0.9375\n",
      "Epoch 607/1000 — loss: 0.2167 — acc: 0.8750\n",
      "Epoch 608/1000 — loss: 0.2381 — acc: 0.8125\n",
      "Epoch 609/1000 — loss: 0.2589 — acc: 0.8125\n",
      "Epoch 610/1000 — loss: 0.2200 — acc: 0.9375\n",
      "Epoch 611/1000 — loss: 0.2021 — acc: 0.8750\n",
      "Epoch 612/1000 — loss: 0.2382 — acc: 0.8750\n",
      "Epoch 613/1000 — loss: 0.2611 — acc: 0.8750\n",
      "Epoch 614/1000 — loss: 0.2402 — acc: 0.8125\n",
      "Epoch 615/1000 — loss: 0.2302 — acc: 0.8125\n",
      "Epoch 616/1000 — loss: 0.2344 — acc: 0.8125\n",
      "Epoch 617/1000 — loss: 0.2403 — acc: 0.8125\n",
      "Epoch 618/1000 — loss: 0.2270 — acc: 0.8750\n",
      "Epoch 619/1000 — loss: 0.2266 — acc: 0.8125\n",
      "Epoch 620/1000 — loss: 0.2827 — acc: 0.8750\n",
      "Epoch 621/1000 — loss: 0.2039 — acc: 0.9375\n",
      "Epoch 622/1000 — loss: 0.2200 — acc: 0.9375\n",
      "Epoch 623/1000 — loss: 0.2559 — acc: 0.8125\n",
      "Epoch 624/1000 — loss: 0.2140 — acc: 0.8750\n",
      "Epoch 625/1000 — loss: 0.2316 — acc: 0.8125\n",
      "Epoch 626/1000 — loss: 0.2347 — acc: 0.8750\n",
      "Epoch 627/1000 — loss: 0.2108 — acc: 0.8750\n",
      "Epoch 628/1000 — loss: 0.2357 — acc: 0.8750\n",
      "Epoch 629/1000 — loss: 0.2334 — acc: 0.8750\n",
      "Epoch 630/1000 — loss: 0.2409 — acc: 0.8750\n",
      "Epoch 631/1000 — loss: 0.2174 — acc: 0.9375\n",
      "Epoch 632/1000 — loss: 0.2225 — acc: 0.8750\n",
      "Epoch 633/1000 — loss: 0.2315 — acc: 0.8750\n",
      "Epoch 634/1000 — loss: 0.2056 — acc: 0.8750\n",
      "Epoch 635/1000 — loss: 0.2272 — acc: 0.8125\n",
      "Epoch 636/1000 — loss: 0.2234 — acc: 0.8125\n",
      "Epoch 637/1000 — loss: 0.2011 — acc: 0.9375\n",
      "Epoch 638/1000 — loss: 0.2332 — acc: 0.8125\n",
      "Epoch 639/1000 — loss: 0.1968 — acc: 1.0000\n",
      "Epoch 640/1000 — loss: 0.2083 — acc: 0.8750\n",
      "Epoch 641/1000 — loss: 0.2400 — acc: 0.8125\n",
      "Epoch 642/1000 — loss: 0.2590 — acc: 0.8125\n",
      "Epoch 643/1000 — loss: 0.2276 — acc: 0.8125\n",
      "Epoch 644/1000 — loss: 0.2164 — acc: 0.8750\n",
      "Epoch 645/1000 — loss: 0.2216 — acc: 0.8750\n",
      "Epoch 646/1000 — loss: 0.1872 — acc: 1.0000\n",
      "Epoch 647/1000 — loss: 0.2045 — acc: 0.9375\n",
      "Epoch 648/1000 — loss: 0.2154 — acc: 0.8125\n",
      "Epoch 649/1000 — loss: 0.2188 — acc: 0.8125\n",
      "Epoch 650/1000 — loss: 0.2301 — acc: 0.8125\n",
      "Epoch 651/1000 — loss: 0.2242 — acc: 0.8125\n",
      "Epoch 652/1000 — loss: 0.2318 — acc: 0.8750\n",
      "Epoch 653/1000 — loss: 0.2154 — acc: 0.9375\n",
      "Epoch 654/1000 — loss: 0.2022 — acc: 0.8750\n",
      "Epoch 655/1000 — loss: 0.2274 — acc: 0.8125\n",
      "Epoch 656/1000 — loss: 0.2361 — acc: 0.8750\n",
      "Epoch 657/1000 — loss: 0.2219 — acc: 0.8750\n",
      "Epoch 658/1000 — loss: 0.1883 — acc: 0.9375\n",
      "Epoch 659/1000 — loss: 0.2123 — acc: 0.9375\n",
      "Epoch 660/1000 — loss: 0.2428 — acc: 0.8125\n",
      "Epoch 661/1000 — loss: 0.2462 — acc: 0.8750\n",
      "Epoch 662/1000 — loss: 0.2284 — acc: 0.8750\n",
      "Epoch 663/1000 — loss: 0.2570 — acc: 0.8125\n",
      "Epoch 664/1000 — loss: 0.2045 — acc: 0.8750\n",
      "Epoch 665/1000 — loss: 0.2174 — acc: 0.9375\n",
      "Epoch 666/1000 — loss: 0.2058 — acc: 0.8750\n",
      "Epoch 667/1000 — loss: 0.1954 — acc: 0.9375\n",
      "Epoch 668/1000 — loss: 0.2098 — acc: 0.8750\n",
      "Epoch 669/1000 — loss: 0.2247 — acc: 0.8125\n",
      "Epoch 670/1000 — loss: 0.2086 — acc: 0.8125\n",
      "Epoch 671/1000 — loss: 0.1786 — acc: 1.0000\n",
      "Epoch 672/1000 — loss: 0.2183 — acc: 0.8750\n",
      "Epoch 673/1000 — loss: 0.2173 — acc: 0.8750\n",
      "Epoch 674/1000 — loss: 0.2136 — acc: 0.8750\n",
      "Epoch 675/1000 — loss: 0.2276 — acc: 0.8125\n",
      "Epoch 676/1000 — loss: 0.2183 — acc: 0.8750\n",
      "Epoch 677/1000 — loss: 0.2068 — acc: 0.8750\n",
      "Epoch 678/1000 — loss: 0.2639 — acc: 0.8125\n",
      "Epoch 679/1000 — loss: 0.2074 — acc: 0.9375\n",
      "Epoch 680/1000 — loss: 0.2134 — acc: 0.8750\n",
      "Epoch 681/1000 — loss: 0.2590 — acc: 0.8125\n",
      "Epoch 682/1000 — loss: 0.2470 — acc: 0.8125\n",
      "Epoch 683/1000 — loss: 0.2139 — acc: 0.8125\n",
      "Epoch 684/1000 — loss: 0.2176 — acc: 0.8750\n",
      "Epoch 685/1000 — loss: 0.2184 — acc: 0.8750\n",
      "Epoch 686/1000 — loss: 0.2124 — acc: 0.8750\n",
      "Epoch 687/1000 — loss: 0.1958 — acc: 0.9375\n",
      "Epoch 688/1000 — loss: 0.1877 — acc: 0.8750\n",
      "Epoch 689/1000 — loss: 0.2346 — acc: 0.8125\n",
      "Epoch 690/1000 — loss: 0.2218 — acc: 0.8125\n",
      "Epoch 691/1000 — loss: 0.2189 — acc: 0.8750\n",
      "Epoch 692/1000 — loss: 0.2229 — acc: 0.8750\n",
      "Epoch 693/1000 — loss: 0.2151 — acc: 0.8750\n",
      "Epoch 694/1000 — loss: 0.2079 — acc: 0.9375\n",
      "Epoch 695/1000 — loss: 0.2516 — acc: 0.8125\n",
      "Epoch 696/1000 — loss: 0.2231 — acc: 0.8750\n",
      "Epoch 697/1000 — loss: 0.2192 — acc: 0.8750\n",
      "Epoch 698/1000 — loss: 0.2046 — acc: 0.8750\n",
      "Epoch 699/1000 — loss: 0.2186 — acc: 0.8750\n",
      "Epoch 700/1000 — loss: 0.2069 — acc: 0.8750\n",
      "Epoch 701/1000 — loss: 0.1991 — acc: 0.9375\n",
      "Epoch 702/1000 — loss: 0.2163 — acc: 0.8750\n",
      "Epoch 703/1000 — loss: 0.1902 — acc: 0.9375\n",
      "Epoch 704/1000 — loss: 0.2220 — acc: 0.8750\n",
      "Epoch 705/1000 — loss: 0.2119 — acc: 0.9375\n",
      "Epoch 706/1000 — loss: 0.2307 — acc: 0.8125\n",
      "Epoch 707/1000 — loss: 0.2403 — acc: 0.8750\n",
      "Epoch 708/1000 — loss: 0.2361 — acc: 0.8125\n",
      "Epoch 709/1000 — loss: 0.2013 — acc: 0.9375\n",
      "Epoch 710/1000 — loss: 0.2213 — acc: 0.8750\n",
      "Epoch 711/1000 — loss: 0.2110 — acc: 0.8750\n",
      "Epoch 712/1000 — loss: 0.2022 — acc: 0.9375\n",
      "Epoch 713/1000 — loss: 0.2087 — acc: 0.8750\n",
      "Epoch 714/1000 — loss: 0.2063 — acc: 0.8125\n",
      "Epoch 715/1000 — loss: 0.2211 — acc: 0.8750\n",
      "Epoch 716/1000 — loss: 0.2129 — acc: 0.9375\n",
      "Epoch 717/1000 — loss: 0.2138 — acc: 0.8750\n",
      "Epoch 718/1000 — loss: 0.2184 — acc: 0.9375\n",
      "Epoch 719/1000 — loss: 0.2108 — acc: 0.8750\n",
      "Epoch 720/1000 — loss: 0.2275 — acc: 0.8125\n",
      "Epoch 721/1000 — loss: 0.1988 — acc: 0.8750\n",
      "Epoch 722/1000 — loss: 0.2356 — acc: 0.8750\n",
      "Epoch 723/1000 — loss: 0.2305 — acc: 0.8125\n",
      "Epoch 724/1000 — loss: 0.2177 — acc: 0.8750\n",
      "Epoch 725/1000 — loss: 0.2395 — acc: 0.8125\n",
      "Epoch 726/1000 — loss: 0.2227 — acc: 0.8750\n",
      "Epoch 727/1000 — loss: 0.2146 — acc: 0.9375\n",
      "Epoch 728/1000 — loss: 0.2312 — acc: 0.8125\n",
      "Epoch 729/1000 — loss: 0.1991 — acc: 0.8750\n",
      "Epoch 730/1000 — loss: 0.2045 — acc: 0.8750\n",
      "Epoch 731/1000 — loss: 0.2424 — acc: 0.8125\n",
      "Epoch 732/1000 — loss: 0.2093 — acc: 0.8750\n",
      "Epoch 733/1000 — loss: 0.2109 — acc: 0.9375\n",
      "Epoch 734/1000 — loss: 0.2486 — acc: 0.8125\n",
      "Epoch 735/1000 — loss: 0.2157 — acc: 0.8750\n",
      "Epoch 736/1000 — loss: 0.1937 — acc: 0.9375\n",
      "Epoch 737/1000 — loss: 0.2102 — acc: 0.8750\n",
      "Epoch 738/1000 — loss: 0.2217 — acc: 0.8750\n",
      "Epoch 739/1000 — loss: 0.2443 — acc: 0.8750\n",
      "Epoch 740/1000 — loss: 0.2155 — acc: 0.8125\n",
      "Epoch 741/1000 — loss: 0.2007 — acc: 0.9375\n",
      "Epoch 742/1000 — loss: 0.2004 — acc: 0.9375\n",
      "Epoch 743/1000 — loss: 0.2461 — acc: 0.8125\n",
      "Epoch 744/1000 — loss: 0.2168 — acc: 0.8750\n",
      "Epoch 745/1000 — loss: 0.2036 — acc: 0.8750\n",
      "Epoch 746/1000 — loss: 0.1995 — acc: 0.8750\n",
      "Epoch 747/1000 — loss: 0.2391 — acc: 0.8125\n",
      "Epoch 748/1000 — loss: 0.1904 — acc: 0.9375\n",
      "Epoch 749/1000 — loss: 0.1989 — acc: 0.9375\n",
      "Epoch 750/1000 — loss: 0.1930 — acc: 0.9375\n",
      "Epoch 751/1000 — loss: 0.2322 — acc: 0.8125\n",
      "Epoch 752/1000 — loss: 0.2004 — acc: 0.8750\n",
      "Epoch 753/1000 — loss: 0.2258 — acc: 0.8750\n",
      "Epoch 754/1000 — loss: 0.2081 — acc: 0.8750\n",
      "Epoch 755/1000 — loss: 0.1971 — acc: 0.9375\n",
      "Epoch 756/1000 — loss: 0.2010 — acc: 0.9375\n",
      "Epoch 757/1000 — loss: 0.2228 — acc: 0.8750\n",
      "Epoch 758/1000 — loss: 0.2230 — acc: 0.8125\n",
      "Epoch 759/1000 — loss: 0.2219 — acc: 0.8125\n",
      "Epoch 760/1000 — loss: 0.2201 — acc: 0.8125\n",
      "Epoch 761/1000 — loss: 0.2122 — acc: 0.8750\n",
      "Epoch 762/1000 — loss: 0.2269 — acc: 0.8750\n",
      "Epoch 763/1000 — loss: 0.2200 — acc: 0.8750\n",
      "Epoch 764/1000 — loss: 0.2465 — acc: 0.8125\n",
      "Epoch 765/1000 — loss: 0.2067 — acc: 0.8125\n",
      "Epoch 766/1000 — loss: 0.1967 — acc: 0.9375\n",
      "Epoch 767/1000 — loss: 0.2043 — acc: 0.8125\n",
      "Epoch 768/1000 — loss: 0.2256 — acc: 0.8125\n",
      "Epoch 769/1000 — loss: 0.1994 — acc: 0.9375\n",
      "Epoch 770/1000 — loss: 0.2467 — acc: 0.8750\n",
      "Epoch 771/1000 — loss: 0.2182 — acc: 0.8750\n",
      "Epoch 772/1000 — loss: 0.2077 — acc: 0.8750\n",
      "Epoch 773/1000 — loss: 0.2028 — acc: 0.9375\n",
      "Epoch 774/1000 — loss: 0.2099 — acc: 0.8125\n",
      "Epoch 775/1000 — loss: 0.2140 — acc: 0.8750\n",
      "Epoch 776/1000 — loss: 0.2256 — acc: 0.8750\n",
      "Epoch 777/1000 — loss: 0.2220 — acc: 0.8750\n",
      "Epoch 778/1000 — loss: 0.2142 — acc: 0.8750\n",
      "Epoch 779/1000 — loss: 0.2158 — acc: 0.9375\n",
      "Epoch 780/1000 — loss: 0.2122 — acc: 0.8750\n",
      "Epoch 781/1000 — loss: 0.2313 — acc: 0.8125\n",
      "Epoch 782/1000 — loss: 0.2082 — acc: 0.8750\n",
      "Epoch 783/1000 — loss: 0.2419 — acc: 0.8750\n",
      "Epoch 784/1000 — loss: 0.2357 — acc: 0.8750\n",
      "Epoch 785/1000 — loss: 0.2195 — acc: 0.8750\n",
      "Epoch 786/1000 — loss: 0.2143 — acc: 0.8125\n",
      "Epoch 787/1000 — loss: 0.2066 — acc: 0.8750\n",
      "Epoch 788/1000 — loss: 0.2140 — acc: 0.8750\n",
      "Epoch 789/1000 — loss: 0.1882 — acc: 0.9375\n",
      "Epoch 790/1000 — loss: 0.2095 — acc: 0.8750\n",
      "Epoch 791/1000 — loss: 0.2463 — acc: 0.8125\n",
      "Epoch 792/1000 — loss: 0.2343 — acc: 0.8125\n",
      "Epoch 793/1000 — loss: 0.1842 — acc: 1.0000\n",
      "Epoch 794/1000 — loss: 0.2312 — acc: 0.8125\n",
      "Epoch 795/1000 — loss: 0.2520 — acc: 0.8125\n",
      "Epoch 796/1000 — loss: 0.2255 — acc: 0.8750\n",
      "Epoch 797/1000 — loss: 0.2006 — acc: 0.9375\n",
      "Epoch 798/1000 — loss: 0.1990 — acc: 0.9375\n",
      "Epoch 799/1000 — loss: 0.1862 — acc: 1.0000\n",
      "Epoch 800/1000 — loss: 0.2311 — acc: 0.8125\n",
      "Epoch 801/1000 — loss: 0.2284 — acc: 0.8125\n",
      "Epoch 802/1000 — loss: 0.2139 — acc: 0.8750\n",
      "Epoch 803/1000 — loss: 0.2180 — acc: 0.9375\n",
      "Epoch 804/1000 — loss: 0.2230 — acc: 0.8750\n",
      "Epoch 805/1000 — loss: 0.2170 — acc: 0.8125\n",
      "Epoch 806/1000 — loss: 0.2226 — acc: 0.8750\n",
      "Epoch 807/1000 — loss: 0.2424 — acc: 0.8125\n",
      "Epoch 808/1000 — loss: 0.2077 — acc: 0.9375\n",
      "Epoch 809/1000 — loss: 0.2275 — acc: 0.8750\n",
      "Epoch 810/1000 — loss: 0.2150 — acc: 0.8750\n",
      "Epoch 811/1000 — loss: 0.2024 — acc: 0.9375\n",
      "Epoch 812/1000 — loss: 0.2125 — acc: 0.8750\n",
      "Epoch 813/1000 — loss: 0.2230 — acc: 0.8750\n",
      "Epoch 814/1000 — loss: 0.2135 — acc: 0.8750\n",
      "Epoch 815/1000 — loss: 0.1980 — acc: 0.8750\n",
      "Epoch 816/1000 — loss: 0.2257 — acc: 0.8125\n",
      "Epoch 817/1000 — loss: 0.2272 — acc: 0.8750\n",
      "Epoch 818/1000 — loss: 0.2480 — acc: 0.8750\n",
      "Epoch 819/1000 — loss: 0.2253 — acc: 0.8125\n",
      "Epoch 820/1000 — loss: 0.2347 — acc: 0.8750\n",
      "Epoch 821/1000 — loss: 0.2019 — acc: 0.8750\n",
      "Epoch 822/1000 — loss: 0.1808 — acc: 1.0000\n",
      "Epoch 823/1000 — loss: 0.2104 — acc: 0.8125\n",
      "Epoch 824/1000 — loss: 0.2437 — acc: 0.8125\n",
      "Epoch 825/1000 — loss: 0.2195 — acc: 0.8750\n",
      "Epoch 826/1000 — loss: 0.2065 — acc: 0.8750\n",
      "Epoch 827/1000 — loss: 0.2174 — acc: 0.8750\n",
      "Epoch 828/1000 — loss: 0.2215 — acc: 0.8125\n",
      "Epoch 829/1000 — loss: 0.2208 — acc: 0.8750\n",
      "Epoch 830/1000 — loss: 0.2334 — acc: 0.8125\n",
      "Epoch 831/1000 — loss: 0.1755 — acc: 0.9375\n",
      "Epoch 832/1000 — loss: 0.2130 — acc: 0.8125\n",
      "Epoch 833/1000 — loss: 0.2183 — acc: 0.8750\n",
      "Epoch 834/1000 — loss: 0.2295 — acc: 0.8125\n",
      "Epoch 835/1000 — loss: 0.2209 — acc: 0.8750\n",
      "Epoch 836/1000 — loss: 0.2074 — acc: 0.8750\n",
      "Epoch 837/1000 — loss: 0.2158 — acc: 0.8125\n",
      "Epoch 838/1000 — loss: 0.2303 — acc: 0.8125\n",
      "Epoch 839/1000 — loss: 0.2022 — acc: 0.8750\n",
      "Epoch 840/1000 — loss: 0.2166 — acc: 0.8750\n",
      "Epoch 841/1000 — loss: 0.2209 — acc: 0.8750\n",
      "Epoch 842/1000 — loss: 0.2458 — acc: 0.8125\n",
      "Epoch 843/1000 — loss: 0.2118 — acc: 0.8750\n",
      "Epoch 844/1000 — loss: 0.2081 — acc: 0.8125\n",
      "Epoch 845/1000 — loss: 0.2300 — acc: 0.8125\n",
      "Epoch 846/1000 — loss: 0.2188 — acc: 0.8125\n",
      "Epoch 847/1000 — loss: 0.2183 — acc: 0.8750\n",
      "Epoch 848/1000 — loss: 0.2350 — acc: 0.8125\n",
      "Epoch 849/1000 — loss: 0.2346 — acc: 0.8125\n",
      "Epoch 850/1000 — loss: 0.2101 — acc: 0.8750\n",
      "Epoch 851/1000 — loss: 0.2252 — acc: 0.8750\n",
      "Epoch 852/1000 — loss: 0.2359 — acc: 0.8750\n",
      "Epoch 853/1000 — loss: 0.2052 — acc: 0.8750\n",
      "Epoch 854/1000 — loss: 0.2851 — acc: 0.8125\n",
      "Epoch 855/1000 — loss: 0.1955 — acc: 0.9375\n",
      "Epoch 856/1000 — loss: 0.2161 — acc: 0.8750\n",
      "Epoch 857/1000 — loss: 0.2044 — acc: 0.9375\n",
      "Epoch 858/1000 — loss: 0.2702 — acc: 0.8125\n",
      "Epoch 859/1000 — loss: 0.2149 — acc: 0.8750\n",
      "Epoch 860/1000 — loss: 0.1969 — acc: 0.8750\n",
      "Epoch 861/1000 — loss: 0.2113 — acc: 0.8750\n",
      "Epoch 862/1000 — loss: 0.2162 — acc: 0.8750\n",
      "Epoch 863/1000 — loss: 0.1961 — acc: 0.8750\n",
      "Epoch 864/1000 — loss: 0.1927 — acc: 0.9375\n",
      "Epoch 865/1000 — loss: 0.2382 — acc: 0.8125\n",
      "Epoch 866/1000 — loss: 0.2102 — acc: 0.8750\n",
      "Epoch 867/1000 — loss: 0.2115 — acc: 0.8750\n",
      "Epoch 868/1000 — loss: 0.1857 — acc: 1.0000\n",
      "Epoch 869/1000 — loss: 0.2167 — acc: 0.8750\n",
      "Epoch 870/1000 — loss: 0.2615 — acc: 0.8125\n",
      "Epoch 871/1000 — loss: 0.2152 — acc: 0.8750\n",
      "Epoch 872/1000 — loss: 0.2233 — acc: 0.8750\n",
      "Epoch 873/1000 — loss: 0.2182 — acc: 0.8750\n",
      "Epoch 874/1000 — loss: 0.2202 — acc: 0.8750\n",
      "Epoch 875/1000 — loss: 0.2028 — acc: 0.8750\n",
      "Epoch 876/1000 — loss: 0.2444 — acc: 0.9375\n",
      "Epoch 877/1000 — loss: 0.2339 — acc: 0.8125\n",
      "Epoch 878/1000 — loss: 0.2032 — acc: 0.8750\n",
      "Epoch 879/1000 — loss: 0.2079 — acc: 0.8750\n",
      "Epoch 880/1000 — loss: 0.1956 — acc: 0.9375\n",
      "Epoch 881/1000 — loss: 0.2251 — acc: 0.8750\n",
      "Epoch 882/1000 — loss: 0.2047 — acc: 0.9375\n",
      "Epoch 883/1000 — loss: 0.2113 — acc: 0.8125\n",
      "Epoch 884/1000 — loss: 0.2195 — acc: 0.8125\n",
      "Epoch 885/1000 — loss: 0.1997 — acc: 0.9375\n",
      "Epoch 886/1000 — loss: 0.2104 — acc: 0.8750\n",
      "Epoch 887/1000 — loss: 0.2228 — acc: 0.8125\n",
      "Epoch 888/1000 — loss: 0.2144 — acc: 0.8750\n",
      "Epoch 889/1000 — loss: 0.2166 — acc: 0.8125\n",
      "Epoch 890/1000 — loss: 0.2294 — acc: 0.8750\n",
      "Epoch 891/1000 — loss: 0.2066 — acc: 0.9375\n",
      "Epoch 892/1000 — loss: 0.2040 — acc: 0.8750\n",
      "Epoch 893/1000 — loss: 0.2039 — acc: 0.9375\n",
      "Epoch 894/1000 — loss: 0.2097 — acc: 0.9375\n",
      "Epoch 895/1000 — loss: 0.2047 — acc: 0.9375\n",
      "Epoch 896/1000 — loss: 0.2386 — acc: 0.8125\n",
      "Epoch 897/1000 — loss: 0.2369 — acc: 0.8125\n",
      "Epoch 898/1000 — loss: 0.2169 — acc: 0.8750\n",
      "Epoch 899/1000 — loss: 0.1979 — acc: 0.9375\n",
      "Epoch 900/1000 — loss: 0.2378 — acc: 0.8750\n",
      "Epoch 901/1000 — loss: 0.2004 — acc: 0.9375\n",
      "Epoch 902/1000 — loss: 0.2149 — acc: 0.8750\n",
      "Epoch 903/1000 — loss: 0.2426 — acc: 0.8125\n",
      "Epoch 904/1000 — loss: 0.2009 — acc: 0.8750\n",
      "Epoch 905/1000 — loss: 0.2178 — acc: 0.9375\n",
      "Epoch 906/1000 — loss: 0.2023 — acc: 0.8750\n",
      "Epoch 907/1000 — loss: 0.2237 — acc: 0.8125\n",
      "Epoch 908/1000 — loss: 0.2216 — acc: 0.8125\n",
      "Epoch 909/1000 — loss: 0.2031 — acc: 0.8750\n",
      "Epoch 910/1000 — loss: 0.1807 — acc: 1.0000\n",
      "Epoch 911/1000 — loss: 0.2145 — acc: 0.8750\n",
      "Epoch 912/1000 — loss: 0.2114 — acc: 0.8125\n",
      "Epoch 913/1000 — loss: 0.2253 — acc: 0.8750\n",
      "Epoch 914/1000 — loss: 0.2206 — acc: 0.8125\n",
      "Epoch 915/1000 — loss: 0.2132 — acc: 0.8125\n",
      "Epoch 916/1000 — loss: 0.2256 — acc: 0.8750\n",
      "Epoch 917/1000 — loss: 0.1888 — acc: 1.0000\n",
      "Epoch 918/1000 — loss: 0.2172 — acc: 0.8750\n",
      "Epoch 919/1000 — loss: 0.1972 — acc: 0.9375\n",
      "Epoch 920/1000 — loss: 0.2006 — acc: 0.9375\n",
      "Epoch 921/1000 — loss: 0.2485 — acc: 0.8125\n",
      "Epoch 922/1000 — loss: 0.1886 — acc: 0.9375\n",
      "Epoch 923/1000 — loss: 0.2006 — acc: 0.8750\n",
      "Epoch 924/1000 — loss: 0.2079 — acc: 0.8750\n",
      "Epoch 925/1000 — loss: 0.1960 — acc: 0.8750\n",
      "Epoch 926/1000 — loss: 0.2249 — acc: 0.8125\n",
      "Epoch 927/1000 — loss: 0.2843 — acc: 0.8750\n",
      "Epoch 928/1000 — loss: 0.2403 — acc: 0.8750\n",
      "Epoch 929/1000 — loss: 0.2235 — acc: 0.8125\n",
      "Epoch 930/1000 — loss: 0.2155 — acc: 0.8750\n",
      "Epoch 931/1000 — loss: 0.2082 — acc: 0.8750\n",
      "Epoch 932/1000 — loss: 0.2579 — acc: 0.8750\n",
      "Epoch 933/1000 — loss: 0.1913 — acc: 1.0000\n",
      "Epoch 934/1000 — loss: 0.2185 — acc: 0.8750\n",
      "Epoch 935/1000 — loss: 0.2356 — acc: 0.8125\n",
      "Epoch 936/1000 — loss: 0.2386 — acc: 0.8125\n",
      "Epoch 937/1000 — loss: 0.2031 — acc: 0.8750\n",
      "Epoch 938/1000 — loss: 0.2024 — acc: 0.9375\n",
      "Epoch 939/1000 — loss: 0.2082 — acc: 0.8750\n",
      "Epoch 940/1000 — loss: 0.1959 — acc: 0.9375\n",
      "Epoch 941/1000 — loss: 0.2264 — acc: 0.8750\n",
      "Epoch 942/1000 — loss: 0.2093 — acc: 0.8750\n",
      "Epoch 943/1000 — loss: 0.2269 — acc: 0.8125\n",
      "Epoch 944/1000 — loss: 0.2332 — acc: 0.8125\n",
      "Epoch 945/1000 — loss: 0.2708 — acc: 0.8750\n",
      "Epoch 946/1000 — loss: 0.2242 — acc: 0.8750\n",
      "Epoch 947/1000 — loss: 0.2038 — acc: 0.9375\n",
      "Epoch 948/1000 — loss: 0.2207 — acc: 0.8125\n",
      "Epoch 949/1000 — loss: 0.2342 — acc: 0.8750\n",
      "Epoch 950/1000 — loss: 0.2137 — acc: 0.8750\n",
      "Epoch 951/1000 — loss: 0.2458 — acc: 0.8125\n",
      "Epoch 952/1000 — loss: 0.2142 — acc: 0.8125\n",
      "Epoch 953/1000 — loss: 0.2300 — acc: 0.8125\n",
      "Epoch 954/1000 — loss: 0.2197 — acc: 0.8125\n",
      "Epoch 955/1000 — loss: 0.2171 — acc: 0.8125\n",
      "Epoch 956/1000 — loss: 0.2132 — acc: 0.8125\n",
      "Epoch 957/1000 — loss: 0.2090 — acc: 0.8750\n",
      "Epoch 958/1000 — loss: 0.2332 — acc: 0.8125\n",
      "Epoch 959/1000 — loss: 0.1934 — acc: 0.9375\n",
      "Epoch 960/1000 — loss: 0.2563 — acc: 0.8125\n",
      "Epoch 961/1000 — loss: 0.2092 — acc: 0.8750\n",
      "Epoch 962/1000 — loss: 0.1917 — acc: 0.9375\n",
      "Epoch 963/1000 — loss: 0.2224 — acc: 0.8750\n",
      "Epoch 964/1000 — loss: 0.2203 — acc: 0.8125\n",
      "Epoch 965/1000 — loss: 0.2095 — acc: 0.9375\n",
      "Epoch 966/1000 — loss: 0.2274 — acc: 0.8125\n",
      "Epoch 967/1000 — loss: 0.2067 — acc: 0.9375\n",
      "Epoch 968/1000 — loss: 0.2097 — acc: 0.8750\n",
      "Epoch 969/1000 — loss: 0.2187 — acc: 0.8750\n",
      "Epoch 970/1000 — loss: 0.2206 — acc: 0.8750\n",
      "Epoch 971/1000 — loss: 0.2206 — acc: 0.8125\n",
      "Epoch 972/1000 — loss: 0.2159 — acc: 0.8750\n",
      "Epoch 973/1000 — loss: 0.2287 — acc: 0.8125\n",
      "Epoch 974/1000 — loss: 0.2082 — acc: 0.8750\n",
      "Epoch 975/1000 — loss: 0.2179 — acc: 0.8750\n",
      "Epoch 976/1000 — loss: 0.2260 — acc: 0.8750\n",
      "Epoch 977/1000 — loss: 0.2087 — acc: 0.9375\n",
      "Epoch 978/1000 — loss: 0.2378 — acc: 0.8125\n",
      "Epoch 979/1000 — loss: 0.2110 — acc: 0.8750\n",
      "Epoch 980/1000 — loss: 0.1986 — acc: 0.8750\n",
      "Epoch 981/1000 — loss: 0.2136 — acc: 0.8750\n",
      "Epoch 982/1000 — loss: 0.2088 — acc: 0.8750\n",
      "Epoch 983/1000 — loss: 0.2015 — acc: 0.8750\n",
      "Epoch 984/1000 — loss: 0.2184 — acc: 0.8125\n",
      "Epoch 985/1000 — loss: 0.2056 — acc: 0.9375\n",
      "Epoch 986/1000 — loss: 0.1738 — acc: 1.0000\n",
      "Epoch 987/1000 — loss: 0.2624 — acc: 0.8125\n",
      "Epoch 988/1000 — loss: 0.2305 — acc: 0.8750\n",
      "Epoch 989/1000 — loss: 0.2043 — acc: 0.9375\n",
      "Epoch 990/1000 — loss: 0.2123 — acc: 0.8750\n",
      "Epoch 991/1000 — loss: 0.2356 — acc: 0.8125\n",
      "Epoch 992/1000 — loss: 0.2207 — acc: 0.8125\n",
      "Epoch 993/1000 — loss: 0.2427 — acc: 0.8125\n",
      "Epoch 994/1000 — loss: 0.2135 — acc: 0.8750\n",
      "Epoch 995/1000 — loss: 0.2147 — acc: 0.8750\n",
      "Epoch 996/1000 — loss: 0.2084 — acc: 0.8125\n",
      "Epoch 997/1000 — loss: 0.2008 — acc: 0.9375\n",
      "Epoch 998/1000 — loss: 0.2111 — acc: 0.8750\n",
      "Epoch 999/1000 — loss: 0.2100 — acc: 0.8750\n",
      "Epoch 1000/1000 — loss: 0.2104 — acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = MiniTransformer(vocab_size=vocab_size, max_len=inputs.shape[1],\n",
    "                        d_model=embed_dim, num_heads=2, num_layers=2, dff=64, rate=0.1)\n",
    "\n",
    "# Dummy call to build weights\n",
    "_ = model(tf.constant(inputs[:2]), training=False)\n",
    "\n",
    "# Custom training with Keras fit requires wrapping to produce (y_true, y_pred)\n",
    "# We can use a tf.data.Dataset\n",
    "batch_size = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, y_labels))\n",
    "dataset = dataset.shuffle(100).batch(batch_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    steps = 0\n",
    "    for batch_x, batch_y in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, _ = model(batch_x, training=True)  # (batch, seq_len, vocab)\n",
    "            loss = masked_loss(batch_y, logits)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        acc = masked_accuracy(batch_y, logits)\n",
    "        epoch_loss += loss.numpy()\n",
    "        epoch_acc += acc.numpy()\n",
    "        steps += 1\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — loss: {epoch_loss/steps:.4f} — acc: {epoch_acc/steps:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ea1c7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers' attn returned: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose an example input (index 0)\n",
    "sample_input = inputs[0:1]  # shape (1, seq_len)\n",
    "logits, all_attn = model(sample_input, training=False)\n",
    "# all_attn: list of attention tensors for each layer, each shape (batch, num_heads, seq_len, seq_len)\n",
    "print(\"Number of layers' attn returned:\", len(all_attn))\n",
    "\n",
    "layer_to_plot = 0\n",
    "head_to_plot = 0\n",
    "attn_matrix = all_attn[layer_to_plot].numpy()[0, head_to_plot]  # (seq_len, seq_len)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(attn_matrix, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(f'Layer {layer_to_plot+1} Head {head_to_plot+1} Attention')\n",
    "plt.xlabel('Key position')\n",
    "plt.ylabel('Query position')\n",
    "plt.xticks(range(seq_len), [index_word.get(id, '') for id in sample_input[0]], rotation=45)\n",
    "plt.yticks(range(seq_len), [index_word.get(id, '') for id in sample_input[0]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27f906e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [2, 3, 4]\n",
      "Padded Input: [[0 2 3 4]]\n",
      "Input text: 'I love deep'\n",
      "Predicted next word: 'learning'\n"
     ]
    }
   ],
   "source": [
    "# corpus = [\n",
    "#     \"i love deep learning\",\n",
    "#     \"i love artificial intelligence\",\n",
    "#     \"deep learning is fun\",\n",
    "#     \"artificial intelligence is cool\",\n",
    "#     \"i love models\",\n",
    "#     \"models learn patterns\"\n",
    "# ]\n",
    "\n",
    "# test_sentence = \"artificial intelligence is\"\n",
    "test_sentence = \"I love deep\"\n",
    "# Convert to sequence\n",
    "test_seq = tokenizer.texts_to_sequences([test_sentence])[0]\n",
    "print(\"Token IDs:\", test_seq)\n",
    "\n",
    "# Pad to same max length\n",
    "test_input = tf.keras.preprocessing.sequence.pad_sequences([test_seq], maxlen=inputs.shape[1], padding='pre')\n",
    "test_input = test_input.astype('int32')\n",
    "print(\"Padded Input:\", test_input)\n",
    "\n",
    "# Get model outputs\n",
    "logits, attn = model(test_input, training=False)\n",
    "# logits shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "# Focus on the last real token position\n",
    "last_token_index = np.where(test_input[0] != 0)[0][-1]\n",
    "pred_logits = logits[0, last_token_index]  # (vocab_size,)\n",
    "\n",
    "# Get predicted token id\n",
    "pred_id = tf.argmax(pred_logits).numpy()\n",
    "pred_word = index_word.get(pred_id, \"<UNK>\")\n",
    "\n",
    "print(f\"Input text: '{test_sentence}'\")\n",
    "print(f\"Predicted next word: '{pred_word}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
